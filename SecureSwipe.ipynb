{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_4c31YZCF8H",
        "outputId": "1aad2315-36fb-40ed-ddc0-2389107df62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHvyZy4zCPln",
        "outputId": "f2499d86-f79f-408d-91bb-c7a671b7c7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.182s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SMdEAaUwHH3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "KmUgX6R3CVGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea20d19b-4d09-46c5-bcf1-b6a706d80571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.147.60.144\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.9s\n",
            "your url is: https://dark-islands-clean.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "import scipy.stats as stats\n",
        "from sklearn.model_selection import learning_curve\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from tabulate import tabulate\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "from patsy import dmatrices\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_val_score,cross_validate\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"CREDIT CARD FRAUD DETECTION\")\n",
        "st.set_option('deprecation.showPyplotGlobalUse', False)\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/creditcard.csv\")\n",
        "data_=data.shape\n",
        "tab1, tab2, tab3 = st.tabs(['Home','Visualization of data','Logistic Regression-Cross validation-Prediction for input from user'])\n",
        "\n",
        "rbs = RobustScaler()\n",
        "data_small = data[['Time','Amount']]\n",
        "data_small = pd.DataFrame(rbs.fit_transform(data_small))\n",
        "data_small.columns = ['scaled_time','scaled_amount']\n",
        "data = pd.concat([data,data_small],axis=1)\n",
        "data.drop(['Time','Amount'],axis=1,inplace=True)\n",
        "\n",
        "non_fraud = data[data['Class']==0]\n",
        "fraud = data[data['Class']==1]\n",
        "new_data= pd.concat([non_fraud,fraud])\n",
        "new_data = new_data.sample(frac=1)\n",
        "\n",
        "target = 'Class'\n",
        "predictors = ['scaled_time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n",
        "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n",
        "       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n",
        "       'scaled_amount']\n",
        "few_cols=['scaled_time', 'V1','V13','V16']\n",
        "\n",
        "def metrics(model_name, y_train, y_test, y_hat_train, y_hat_test):\n",
        "    '''Print out the evaluation metrics for a given models predictions'''\n",
        "\n",
        "    st.write(f'Model: {model_name}', )\n",
        "    st.write('-'*60)\n",
        "    # Calculate confusion matrix\n",
        "    cm_test = confusion_matrix(y_test, y_hat_test)\n",
        "    cm_train = confusion_matrix(y_train, y_hat_train)\n",
        "\n",
        "    #plot_confusion_matrix(y_test,y_hat_test)\n",
        "    # Display confusion matrix for Test Data\n",
        "    st.write('Confusion Matrix for Test Data:')\n",
        "    printd=pd.DataFrame(cm_test, columns=[\"Predicted 0\", \"Predicted 1\"], index=[\"Actual 0\", \"Actual 1\"])\n",
        "    st.write(printd)\n",
        "    # Display confusion matrix as a plot\n",
        "    plt.figure(figsize=(2, 1))\n",
        "    sns.heatmap(cm_test, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "    st.pyplot()\n",
        "    # Display confusion matrix for Train Data\n",
        "    st.write('\\n\\nConfusion Matrix for Train Data:')\n",
        "    printd=pd.DataFrame(cm_train, columns=[\"Predicted 0\", \"Predicted 1\"], index=[\"Actual 0\", \"Actual 1\"])\n",
        "    # Display confusion matrix for the training data as a plot\n",
        "    plt.figure(figsize=(2, 1))\n",
        "    sns.heatmap(cm_train, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "    st.pyplot()\n",
        "    st.write(f'Test accuracy: {round(accuracy_score(y_test, y_hat_test),2)}')\n",
        "    st.write(f'Train accuracy: {round(accuracy_score(y_train, y_hat_train),2)}')\n",
        "    st.write('-'*60)\n",
        "    st.write('-'*60)\n",
        "    st.write('Confusion Matrix:\\n',pd.crosstab(y_test, y_hat_test,rownames=['Actual'],colnames=['Predicted'],margins = True))\n",
        "    st.write('\\nTest report:\\n' + classification_report(y_test, y_hat_test))\n",
        "    st.write('~'*60)\n",
        "    st.write('\\nTrain report:\\n' + classification_report(y_train, y_hat_train))\n",
        "    st.write('-'*60)\n",
        "    st.write('\\n\\n')\n",
        "\n",
        "def boxplot_(new_data):\n",
        "    #outlier detection\n",
        "    plt.style.use('dark_background')\n",
        "    for var in few_cols:\n",
        "        sns.boxplot(x=new_data[var],hue=new_data['Class'], palette='Set3')\n",
        "        mean = new_data[var].mean()\n",
        "        std = new_data[var].std()\n",
        "        plt.axvline(mean - 3 * std, 0, 1)\n",
        "        plt.text(mean - 3 * std, -0.55, 'mean - 3* std', rotation=60)\n",
        "        plt.axvline(mean + 3 * std, 0, 1)\n",
        "        plt.text(mean + 3 * std, -0.55, 'mean + 3* std', rotation=60)\n",
        "        plt.show()\n",
        "        st.pyplot()\n",
        "\n",
        "def IQR_method(df,n,features):\n",
        "    outlier_list = []\n",
        "\n",
        "    for column in features:\n",
        "        Q1 = np.percentile(df[column], 25)\n",
        "        Q3 = np.percentile(df[column],75)\n",
        "        IQR = Q3 - Q1\n",
        "        outlier_step = 1.5 * IQR\n",
        "        outlier_list_column = df[(df[column] < Q1 - outlier_step) | (df[column] > Q3 + outlier_step )].index\n",
        "        outlier_list.extend(outlier_list_column)\n",
        "\n",
        "    #selecting observations containing more than x outliers\n",
        "    outlier_list = Counter(outlier_list)\n",
        "    multiple_outliers = list( k for k, v in outlier_list.items() if v > n )\n",
        "\n",
        "    out1 = df[df[column] < Q1 - outlier_step]\n",
        "    out2 = df[df[column] > Q3 + outlier_step]\n",
        "\n",
        "    print('Total number of deleted outliers is:', out1.shape[0]+out2.shape[0])\n",
        "\n",
        "    return multiple_outliers\n",
        "\n",
        "def pair_plot(new_data):\n",
        "        plt.style.use('dark_background')\n",
        "        sns.pairplot(new_data,hue='Class',vars=['scaled_time','scaled_amount'], palette='husl')\n",
        "        st.pyplot()\n",
        "def heatmap_(new_data):\n",
        "        plt.style.use('dark_background')\n",
        "        corr = new_data.corr()\n",
        "        plt.figure(figsize=(25, 20))\n",
        "        sns.heatmap(data=corr, annot=True, square=True, cbar=True)\n",
        "        st.pyplot()\n",
        "def histplot_(data):\n",
        "        plt.figure(figsize=(10,6))\n",
        "        sns.histplot(new_data['scaled_time'], bins=50, kde=True)\n",
        "        plt.title('Distribution of Time')\n",
        "        plt.show()\n",
        "        st.pyplot()\n",
        "def displot_(new_data):\n",
        "        plt.style.use('dark_background')\n",
        "        plt.figure(figsize=(10,6))\n",
        "        sns.displot(new_data[new_data['Class'] == 1]['scaled_amount'], bins=50, label='Fraud', kde=False,color='green')\n",
        "        plt.title('Distribution of Fraudulent Transactions')\n",
        "        plt.xlabel('Amount')\n",
        "        plt.ylabel('Number of Transactions')\n",
        "        plt.show()\n",
        "        st.pyplot()\n",
        "def histplot__(new_data):\n",
        "        p99 = new_data[\"scaled_amount\"].quantile(0.99)\n",
        "        sns.histplot(x=\"scaled_amount\", hue=\"Class\", bins=30,\n",
        "             stat=\"probability\", data=new_data[new_data[\"scaled_amount\"] <= p99],\n",
        "             common_norm=False);\n",
        "        st.pyplot()\n",
        "def scatter_plot(new_data):\n",
        "        f,(ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
        "        f.suptitle('Time of transaction vs Amount by class')\n",
        "        ax1.scatter(fraud.scaled_time,fraud.scaled_amount)\n",
        "        ax1.set_title('Fraud')\n",
        "        ax2.scatter(non_fraud.scaled_time, non_fraud.scaled_amount)\n",
        "        ax2.set_title('Non_fraud')\n",
        "        plt.xlabel('Time (in Seconds)')\n",
        "        plt.ylabel('Amount')\n",
        "        plt.show();\n",
        "        st.pyplot()\n",
        "def check_normality(feature):\n",
        "        plt.figure(figsize=(4, 4))\n",
        "        ax1 = plt.subplot(1, 1, 1)\n",
        "        stats.probplot(new_data[feature], dist=stats.norm, plot=ax1)\n",
        "        ax1.set_title(f'{feature} Q-Q plot', fontsize=7)\n",
        "        sns.despine()\n",
        "\n",
        "        mean = new_data[feature].mean()\n",
        "        std = new_data[feature].std()\n",
        "        skew = new_data[feature].skew()\n",
        "        st.write(f'{feature} : mean: {mean:.2f}, std: {std:.2f}, skew: {skew:.2f}')\n",
        "        st.pyplot()\n",
        "\n",
        "def time_density_plot(new_data):\n",
        "        class_0 = new_data.loc[new_data['Class'] == 0][\"scaled_time\"]\n",
        "        class_1 = new_data.loc[new_data['Class'] == 1][\"scaled_time\"]\n",
        "\n",
        "        hist_data = [class_0, class_1]\n",
        "        group_labels=[\"Non Fraud\",\"Fraud\"]\n",
        "        fig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\n",
        "        fig['layout'].update(title='Credit Card Transactions Time Density Plot', xaxis=dict(title='Time [s]'))\n",
        "        iplot(fig, filename='dist_only')\n",
        "        st.pyplot()\n",
        "\n",
        "def pie(new_data):\n",
        "        new_data['Class'].value_counts()\n",
        "        new_data['Class'].value_counts().plot.pie(explode=[0.1,0],autopct='%3.1f%%'\n",
        "                                            , shadow=True, legend= True,startangle =45)\n",
        "        plt.title('Distribution of Class',size=14)\n",
        "        st.write('Genuine:', round(new_data['Class'].value_counts()[0]/len(new_data) * 100,2), '% of the dataset')\n",
        "        st.write('Frauds:', round(new_data['Class'].value_counts()[1]/len(new_data) * 100,2), '% of the dataset')\n",
        "        plt.show()\n",
        "        st.pyplot()\n",
        "\n",
        "def pie_plot(oversampled_data):\n",
        "        oversampled_data['Class'].value_counts().plot.pie(explode=[0.1,0],autopct='%3.1f%%'\n",
        "                                     ,shadow=True, legend= True,startangle =45)\n",
        "        plt.title('Distribution of Class',size=14)\n",
        "        plt.show()\n",
        "        st.pyplot()\n",
        "\n",
        "def feature_importance(tmp):\n",
        "        # Sort the features by absolute coefficient value in descending order\n",
        "        tmp['Abs_Coefficient'] = tmp['Coefficient'].abs()\n",
        "        tmp = tmp.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(8,5))\n",
        "        plt.title('Feature Importances', fontsize=14)\n",
        "        s = sns.barplot(x='Feature', y='Abs_Coefficient', data=tmp)\n",
        "        s.set_xticklabels(s.get_xticklabels(), rotation=90)\n",
        "        plt.show()\n",
        "        st.pyplot()\n",
        "\n",
        "def cross_val(cvs):\n",
        "        plt.plot(range(1, len(cvs) + 1), cvs, \"o-\")\n",
        "        plt.title(\"Cross Val Score\")\n",
        "        plt.show()\n",
        "        st.pyplot()\n",
        "\n",
        "def partition(new_data):\n",
        "        k = 5\n",
        "        samples_per_partition = len(new_data) // k\n",
        "        partitions = []\n",
        "        # Split the dataset into k partitions with equal sample sizes\n",
        "        start_index = 0\n",
        "        for i in range(k):\n",
        "            end_index = start_index + samples_per_partition\n",
        "            partition = new_data.iloc[start_index:end_index]\n",
        "            partitions.append(partition)\n",
        "            start_index = end_index\n",
        "        # Now, 'partitions' contains k DataFrames with equal sample sizes\n",
        "        return partitions\n",
        "def add_gaussian_noise(new_data, noise_std=0.1):\n",
        "    noisy_df = new_data.copy()\n",
        "    for col in new_data.columns:\n",
        "        noisy_df[col] = new_data[col] + np.random.normal(0, noise_std, size=len(new_data))\n",
        "    return noisy_df\n",
        "\n",
        "with tab1:\n",
        "    st.header('Fraud Detection')\n",
        "    st.table(data.head(10))\n",
        "    st.header('Credit Card Data description')\n",
        "    df=data.describe()\n",
        "    st.table(df)\n",
        "    st.header('Details')\n",
        "    st.subheader(\"Checking null values\")\n",
        "    temp=data.isnull().sum()\n",
        "    st.table(temp)\n",
        "    st.subheader('Shape of data')\n",
        "    temp=data.shape\n",
        "    st.subheader(\"Duplicated data\")\n",
        "    # Check for and count duplicated rows\n",
        "    duplicated_count = data.duplicated().sum()\n",
        "    st.write(\"Number of duplicated rows:\", duplicated_count)\n",
        "    st.write('Before processing',data_)\n",
        "    st.write('After preprocessing',temp)\n",
        "    data = new_data.drop_duplicates(keep='first')\n",
        "    st.write(\"After preprocessing-Duplicated sum\",data.duplicated().sum())\n",
        "    total = new_data.isnull().sum().sort_values(ascending = False)\n",
        "    percent = (new_data.isnull().sum()/new_data.isnull().count()*100).sort_values(ascending = False)\n",
        "    pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()\n",
        "    Outliers_IQR = IQR_method(new_data,1,predictors)\n",
        "    st.subheader(\"Outlier's IQR\")\n",
        "    st.write(Outliers_IQR)\n",
        "\n",
        "with tab2:\n",
        "    new_data = new_data.drop_duplicates(keep='first')\n",
        "    st.subheader(\"Boxplot for detecting outliers\")\n",
        "    boxplot_(new_data)\n",
        "    st.header(\"Visualization of Data\")\n",
        "    st.subheader(\"Heatmap \")\n",
        "    heatmap_(new_data)\n",
        "    st.subheader(\"Pair-plot\")\n",
        "    pair_plot(new_data)\n",
        "    st.subheader(\"Histogram for scaled time\")\n",
        "    histplot_(new_data)\n",
        "    st.subheader(\"Displot \")\n",
        "    displot_(new_data)\n",
        "    st.subheader(\"Histplot for scaled amt\")\n",
        "    histplot__(new_data)\n",
        "    st.subheader(\"Scatter plot\")\n",
        "    scatter_plot(new_data)\n",
        "    st.subheader(\"Checking normality\")\n",
        "    check_normality(\"scaled_amount\")\n",
        "    st.subheader('Credit card transactions time density plot')\n",
        "    time_density_plot(new_data)\n",
        "    X = new_data.drop(columns='Class', axis=1)\n",
        "    Y = new_data['Class']\n",
        "    X.isnull().sum()\n",
        "    total = X.isnull().sum().sort_values(ascending = False)\n",
        "    percent = (X.isnull().sum()/X.isnull().count()*100).sort_values(ascending = False)\n",
        "    st.write(pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose())\n",
        "    st.subheader(\"Oversampling using smote\")\n",
        "    st.write('Genuine:', round(new_data['Class'].value_counts()[0]/len(new_data) * 100,2), '% of the dataset')\n",
        "    st.write('Frauds:', round(new_data['Class'].value_counts()[1]/len(new_data) * 100,2), '% of the dataset')\n",
        "    pie(new_data)\n",
        "    st.write(X.shape,Y.shape)\n",
        "    smote = SMOTE(random_state=42)\n",
        "    x_smote, y_smote = smote.fit_resample(X, Y)\n",
        "    st.write(y_smote.value_counts())\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(x_smote, y_smote,test_size=0.20, random_state=42)\n",
        "    oversampled_data = pd.DataFrame(data=x_smote,columns=x_smote.columns)\n",
        "    oversampled_data['Class']=y_smote\n",
        "    oversampled_data.shape\n",
        "    oversampled_data = oversampled_data.sample(frac=1)\n",
        "    st.write(Counter(y_smote),oversampled_data.shape)\n",
        "    st.write('Genuine:', round(oversampled_data['Class'].value_counts()[0]/len(oversampled_data) * 100,2), '% of the dataset')\n",
        "    st.write('Frauds:', round(oversampled_data['Class'].value_counts()[1]/len(oversampled_data) * 100,2), '% of the dataset')\n",
        "    st.subheader('Pie plot after oversampling ')\n",
        "    pie_plot(oversampled_data)\n",
        "\n",
        "with tab3:\n",
        "    new_data = new_data.drop_duplicates(keep='first')\n",
        "    smote = SMOTE(random_state=42)\n",
        "    x_smote, y_smote = smote.fit_resample(X, Y)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(x_smote, y_smote,test_size=0.20, random_state=42)\n",
        "    oversampled_data = pd.DataFrame(data=x_smote,columns=x_smote.columns)\n",
        "    oversampled_data['Class']=y_smote\n",
        "    oversampled_data.shape\n",
        "    oversampled_data = oversampled_data.sample(frac=1)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, stratify=Y, random_state=2)\n",
        "    sc = StandardScaler()\n",
        "    sc.fit(X_train)\n",
        "    X_train= sc.transform(X_train)\n",
        "    X_test = sc.transform(X_test)\n",
        "    st.write('After standard scaling',X.shape, X_train.shape, X_test.shape)\n",
        "    st.subheader('Checking for multicollinearity')\n",
        "    #a DataFrame to hold VIF values\n",
        "    vif_df = pd.DataFrame()\n",
        "    vif_df['variable'] = X.columns\n",
        "\n",
        "    #VIF for each predictor variable\n",
        "    vif_df['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "    st.write(vif_df)\n",
        "\n",
        "    st.subheader('Removing multicollinearity with ridge regression')\n",
        "    ridge = Ridge(alpha=1.0)\n",
        "    ridge.fit(X_train, Y_train)\n",
        "    variable_names = [\"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \"v10\",\n",
        "                    \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \"v18\", \"v19\", \"v20\",\n",
        "                    \"v21\", \"v22\", \"v23\", \"v24\", \"v25\", \"v26\", \"v27\", \"v28\", \"scaled_time\",\"scaled_amount\"]\n",
        "    ridge = Ridge(alpha=1.0)\n",
        "    ridge.fit(X_train, Y_train)\n",
        "\n",
        "    # Coefficients after Ridge regularization\n",
        "    coefficients = ridge.coef_\n",
        "    for i, var in enumerate(variable_names):\n",
        "        st.write(f'{var}: {coefficients[i]}')\n",
        "    Y_train = Y_train.values.ravel()\n",
        "    st.header(\"MODEL\")\n",
        "\n",
        "    st.subheader(\"Logistic Regression\")\n",
        "    model = LogisticRegression(random_state=42,max_iter=1000)\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    st.subheader('Feature Importance')\n",
        "    coefficients = model.coef_[0]\n",
        "\n",
        "    # a DataFrame to store feature names and their corresponding coefficients\n",
        "    tmp = pd.DataFrame({'Feature': predictors, 'Coefficient': coefficients})\n",
        "    feature_importance(tmp)\n",
        "\n",
        "    st.subheader('Model prediction')\n",
        "    Y_pred = model.predict(X_train)\n",
        "    training_data_accuracy = accuracy_score(Y_pred,Y_train)\n",
        "    st.write('Accuracy on Training data : ', training_data_accuracy)\n",
        "    X_test_prediction = model.predict(X_test)\n",
        "    test_data_accuracy = accuracy_score(X_test_prediction, Y_test)\n",
        "    st.write('Accuracy score on Test Data : ', test_data_accuracy)\n",
        "\n",
        "    models = []\n",
        "    models.append(('LR', LogisticRegression(max_iter=1000)))\n",
        "\n",
        "    for name, model in models:\n",
        "        model.fit(X_train, Y_train)\n",
        "        Y_hat_test = model.predict(X_test).astype(int)\n",
        "        Y_hat_train = model.predict(X_train).astype(int)\n",
        "        st.write(name, 'Accuracy Score is : ', accuracy_score(Y_test, Y_hat_test))\n",
        "        st.write(Y_hat_test.shape,Y_hat_train.shape)\n",
        "        st.write(Y_hat_test)\n",
        "        metrics(name, Y_train, Y_test, Y_hat_train, Y_hat_test)\n",
        "\n",
        "    st.subheader('Cross validation')\n",
        "    cv = RepeatedKFold(n_splits=5, n_repeats= 100, random_state=1)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "    scores = cross_validate(model, X_train, Y_train, scoring='accuracy', cv=5)\n",
        "    st.write(scores['test_score'])\n",
        "    st.write('Mean=',np.mean(scores['test_score']))\n",
        "    cvs = cross_val_score(model,X,Y, cv=5, scoring=\"f1_macro\")\n",
        "\n",
        "    model.fit(X_train, Y_train)\n",
        "    transaction_data = pd.DataFrame({\n",
        "    'Time': [16],\n",
        "    'V1': [1.322707269],\n",
        "    'V2': [-0.174040833],\n",
        "    'V3': [0.434555031],\n",
        "    'V4': [0.576037652],\n",
        "    'V5': [-0.836758046],\n",
        "    'V6': [-0.831083411],\n",
        "    'V7': [-0.264904961],\n",
        "    'V8': [-0.220981943],\n",
        "    'V9': [-1.071424618],\n",
        "    'V10': [0.868558548],\n",
        "    'V11': [-0.64150629],\n",
        "    'V12': [-0.111315775],\n",
        "    'V13': [0.36148541],\n",
        "    'V14': [0.171945122],\n",
        "    'V15': [0.782166532],\n",
        "    'V16': [-1.35587073],\n",
        "    'V17': [-0.216935153],\n",
        "    'V18': [1.271765385],\n",
        "    'V19': [-1.240621935],\n",
        "    'V20': [-0.522950941],\n",
        "    'V21': [-0.284375572],\n",
        "    'V22': [-0.323357411],\n",
        "    'V23': [-0.037709905],\n",
        "    'V24': [0.347150939],\n",
        "    'V25': [0.559639137],\n",
        "    'V26': [-0.280158166],\n",
        "    'V27': [0.042335258],\n",
        "    'V28': [0.0288223],\n",
        "    'Amount': [0],\n",
        "    'Class': [0]\n",
        "  })\n",
        "\n",
        "    new_predictions = model.predict(X)\n",
        "\n",
        "    if new_predictions[0] == 1:\n",
        "        st.write(\"Time: 16\\nPredict: Fraudulent Transaction\")\n",
        "    else:\n",
        "        st.write(\"Time: 16\\nPredict: Genuine Transaction\")\n",
        "\n",
        "    st.subheader('PATE')\n",
        "    st.write(\"Training : \",X_train.shape,Y_train.shape)\n",
        "    st.write(\"Testing : \",X_test.shape,Y_test.shape)\n",
        "\n",
        "    def private_aggregate_teacher_predictions(oversampled_data, num_teachers, epsilon):\n",
        "        x = oversampled_data.drop(columns=['Class'])\n",
        "        y = oversampled_data['Class']\n",
        "\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        teacher_models = []\n",
        "\n",
        "        for i in range(num_teachers):\n",
        "            x_train_teacher, x_test_teacher, y_train_teacher, y_test_teacher = train_test_split(x_train, y_train, test_size=0.5, random_state=i)\n",
        "\n",
        "            teacher_model = LogisticRegression(max_iter=1000)\n",
        "            teacher_model.fit(x_train_teacher, y_train_teacher)\n",
        "            teacher_models.append(teacher_model)\n",
        "\n",
        "            teacher_accuracy = accuracy_score(y_test_teacher,teacher_model.predict(x_test_teacher))\n",
        "            st.write(f\"Teacher {i} accuracy: {teacher_accuracy}\")\n",
        "\n",
        "        teacher_predictions = []\n",
        "\n",
        "        for teacher_model in teacher_models:\n",
        "            teacher_pred = teacher_model.predict(x_test)\n",
        "            teacher_predictions.append(teacher_pred)\n",
        "\n",
        "        final_aggregated_predictions = []\n",
        "\n",
        "        for i in range(len(x_test)):\n",
        "            class_counts = [0, 0]\n",
        "\n",
        "            for teacher_prediction in teacher_predictions:\n",
        "                class_counts[teacher_prediction[i]] += 1\n",
        "\n",
        "            #adding noise\n",
        "            noisy_counts = [count + np.random.laplace(scale=1/epsilon) for count in class_counts]\n",
        "\n",
        "            max_votes = max(noisy_counts)\n",
        "            selected_classes = [j for j, vote in enumerate(noisy_counts) if vote == max_votes]\n",
        "            final_prediction = np.random.choice(selected_classes)\n",
        "\n",
        "            final_aggregated_predictions.append(final_prediction)\n",
        "\n",
        "        aggregated_accuracy = accuracy_score(y_test,final_aggregated_predictions)\n",
        "        st.write(f\"Aggregated predictions accuracy: {aggregated_accuracy}\")\n",
        "\n",
        "        return final_aggregated_predictions\n",
        "\n",
        "    num_teachers = 5\n",
        "    epsilon = 1.0\n",
        "    aggregated_predictions = private_aggregate_teacher_predictions(oversampled_data, num_teachers, epsilon)\n",
        "    st.write(\"Final aggregated predictions for all test data points:\", aggregated_predictions)\n",
        "\n",
        "    st.subheader('Federated Learning')\n",
        "    def sigmoid(z):\n",
        "        return 1 / (1 + np.exp(-np.clip(z, -15, 15)))\n",
        "\n",
        "    x = oversampled_data.drop(columns=['Class'])\n",
        "    y = oversampled_data['Class']\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "    sample_fraction = 0.5\n",
        "    x_train = x_train.sample(frac=sample_fraction, random_state=42)\n",
        "    y_train = y_train.loc[x_train.index]\n",
        "\n",
        "    num_clients = 2\n",
        "\n",
        "    global_weights = np.zeros(x_train.shape[1])\n",
        "\n",
        "    client_weights = [np.zeros(x_train.shape[1]) for _ in range(num_clients)]\n",
        "\n",
        "    num_epochs = 1\n",
        "    learning_rate = 0.1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for client_id in range(num_clients):\n",
        "            local_weights = client_weights[client_id]\n",
        "            for i in range(len(x_train)):\n",
        "                x_i = x_train.iloc[i].values\n",
        "                y_i = y_train.iloc[i]\n",
        "                z = np.dot(x_i, local_weights)\n",
        "                h = sigmoid(z)\n",
        "                gradient = (h-y_i) * x_i\n",
        "                local_weights -= learning_rate * gradient\n",
        "\n",
        "            # Print log loss for each client after the first epoch\n",
        "            if epoch == 0 or epoch==1:\n",
        "                z_values = np.dot(x_train.values, local_weights)\n",
        "                h_values = sigmoid(z_values)\n",
        "                loss = log_loss(y_train,h_values)\n",
        "                accuracy = accuracy_score(y_train,(h_values >= 0.5).astype(int))\n",
        "                st.write(f'Client {client_id} - Log Loss: {loss}, Accuracy: {accuracy}')\n",
        "\n",
        "            client_weights[client_id] = local_weights\n",
        "\n",
        "        global_weights = np.mean(client_weights, axis=0)\n",
        "\n",
        "    z_values = np.dot(x_test.values,global_weights)\n",
        "    h_values = sigmoid(z_values)\n",
        "\n",
        "    global_predictions = (h_values >= 0.5).astype(int)\n",
        "    st.write(\"Global Predictions : \",global_predictions)\n",
        "\n",
        "    global_accuracy = accuracy_score(y_test, global_predictions)\n",
        "    st.write(\"Global Model Accuracy: \", global_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIdXCoPXDd8Y",
        "outputId": "5fd0eed6-fe82-4f4f-a066-84ab6d8a27bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    }
  ]
}